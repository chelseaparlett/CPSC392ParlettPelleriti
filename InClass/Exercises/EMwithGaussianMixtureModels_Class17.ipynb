{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Together\n",
    "\n",
    "Expectation Maximization with Gaussian Mixture Models (we'll call it EM for short here) is a clustering algorithm that's similar to k-means except it doesn't assume spherical variance within clusters. That means clusters can be ellipses rather than just sphereical. For example the graph on the left shows roughly spherical clusters, whereas the graph on the right shows non-spherical clusters.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1BslkqKXSuxYNcpAhFLlsVsBSdWUCWY3W\"/>\n",
    "\n",
    "The process for fitting EM is similar to k-means except for two main differences:\n",
    "\n",
    "1. Instead of estimating ONLY cluster means/centers, we also estimate the variance for each predictor.\n",
    "2. Instead of hard assignment (where each data point belongs to only 1 cluster), GMMs use soft assignment (where each data point has a probability of being in EACH cluster). \n",
    "    - because there is no hard assignment, the cluster centers/means and variances are calculated using EVERY data point weighted by the probability that the data point belongs to that cluster. Data points that are unlikely to belong to a cluster barely affect the center/mean and variance of that cluster, whereas data points that are very likely to belong to a cluster have a larger influence on the center/mean and variance of that cluster.\n",
    "\n",
    "This means that when clusters are NOT spherical, EM will be able to accomodate that, while k-means will not.\n",
    "\n",
    "\n",
    "# 1. Comparing K-Means and EM\n",
    "\n",
    "Let's compare the performance of K-Means and EM on different data sets (data is ALREADY z scored). For each dataset listed below, perform K-Means AND EM. For both algorithms, choose the optimal number of clusters by maximizing the silhouette scores. Then answer the following questions:\n",
    "\n",
    "1. is the number of clusters chosen the same for KM and EM?\n",
    "2. how does cluster membership compare between the clusters found by KM and EM?\n",
    "3. make scatterplots of the clusters from KM and from EM\n",
    "4. did you notice any interesting or counterintuitive results?\n",
    "5. did one method have a lower silhouette score than the other?\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Oblong Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/KMEM1.csv\")\n",
    "\n",
    "# perform K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Spread Out Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/KMEM2.csv\")\n",
    "\n",
    "# perform K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Very Distinct Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/KMEM3.csv\")\n",
    "\n",
    "# perform K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cluster in Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/KMEM4.csv\")\n",
    "\n",
    "# perform K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Uneven Cluster Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d5 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/KMEM5.csv\")\n",
    "\n",
    "# perform K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Clusters with Different Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d6 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/KMEM6.csv\")\n",
    "\n",
    "# perform K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Reflection\n",
    "\n",
    "What cautions will you now take when doing K-Means? In other words, what issues did this classwork present that might change how you apply clustering to real data?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recovering TRUE Cluster Membership\n",
    "\n",
    "Normally when performing clustering, we don't have \"truth\" labels that tell us which group each data point is in. But in this exercise we're going to perform clustering on (fake) data that *does* have truth labels and see whether K-Means or EM is better at recovering the true cluster membership (i.e. is better at clustering data points in the same cluster together). \n",
    "\n",
    "The images below show the TRUE cluster/group memberships used to generate the data in 1.1-1.6 above. Use these graphs to answer the following question:\n",
    "\n",
    "### *Question*\n",
    "\n",
    "- Are there any issues with the number of clusters chosen? Is this number often correct? \n",
    "\n",
    "- Do you notice that there are times when EM is better at recovering cluster membership?\n",
    "\n",
    "- What kind of clusters/data do you think EM and KM would perform *equally* well on? When would EM likely do better?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n",
    "### 1.1\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1D9fqveUI0tiga6JrBZH8ilI_m8RwmFax\" width = 500px />\n",
    "\n",
    "### 1.2\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vs0zDmacrTKLONnocUvvZ2pv96qoDsoE\" width = 500px />\n",
    "\n",
    "### 1.3\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1fYD1gLNyEZ9k6nMho9P5Jal1t8xw4tN0\" width = 500px />\n",
    "\n",
    "### 1.4\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1BL1Nh08IA1xn0g4vkokoCysQp71hXxf9\" width = 500px />\n",
    "\n",
    "### 1.5\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1XrPKzDgDK4dson0oMEVjRTHAqgagdmWf\" width = 500px />\n",
    "\n",
    "### 1.6\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vWEsJqBYoa4nl_eqWUmQbYzWhJrszk8F\" width = 500px />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cluster Stability\n",
    "\n",
    "You may have already noticed this, but K-Means and EM will often give different solutions each time it runs. Run the following cells multiple times and notice how different (or not) the results are. What do you think could cause this instability?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters = 9)\n",
    "KM.fit(d6)\n",
    "pred = KM.predict(d6)\n",
    "ggplot(d6, aes(\"x\", \"y\", color = pred)) + geom_point() + theme_minimal() + theme(legend_position = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM = GaussianMixture(n_components = 9)\n",
    "GM.fit(d5)\n",
    "pred = GM.predict(d5)\n",
    "ggplot(d5, aes(\"x\", \"y\", color = pred)) + geom_point() + theme_minimal() + theme(legend_position = \"none\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Chelsea's Thoughts\n",
    "\n",
    "I hope this classwork doesn't scare you away from clustering. Clustering is an incredibly useful tool! However, it's not a perfect tool, and like all the other models we've learned, you have to be careful and thoughtful in how you apply it. \n",
    "\n",
    "I love [this](https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means/133694#133694) stackOverflow thread about k-means if you want to delve deeper."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
