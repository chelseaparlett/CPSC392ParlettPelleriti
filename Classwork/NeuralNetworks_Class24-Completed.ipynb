{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as kb\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Together\n",
    "\n",
    "Neural Networks are great. Their flexibility (layers...connections...activation functions...and more!) allows you to build complex models that can accurately model complex relationships between predictors and outcomes. But I want to caution you: Neural Networks aren't magic. I often see people using them unnecessarily, just because they sound cool. If you're going to use NN's, make sure they're the right tool for your problem.\n",
    "\n",
    "\n",
    "When building a neural network you need to think about 2 (main) things:\n",
    "\n",
    "1. The Structure of the model (nodes/connections/activation functions)\n",
    "2. The Loss Function (how do we measure how well our model is doing?)\n",
    "\n",
    "\n",
    "# 1. Building a Simple NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2553, 14)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/Music_data.csv\")\n",
    "feats = [\"danceability\", \"energy\", \"loudness\",\"acousticness\"]\n",
    "predict = \"valence\"\n",
    "\n",
    "print(df.shape)\n",
    "X = df[feats]\n",
    "y = df[predict]\n",
    "\n",
    "z = StandardScaler()\n",
    "\n",
    "X = z.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model below has the same shape as a simple linear regression, like we talked about in lecture. It has an input layer with 4 inputs (\"danceability\", \"energy\", \"loudness\",\"acousticness\"), and 1 output layer for \"valence\".\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=16JR3yX1gs7oC1isJAaJixNkrnZmdOxn1\" width = 800px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2553/2553 [==============================] - 0s 51us/step - loss: 0.4963\n",
      "Epoch 2/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0670\n",
      "Epoch 3/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0469\n",
      "Epoch 4/100\n",
      "2553/2553 [==============================] - 0s 20us/step - loss: 0.0405\n",
      "Epoch 5/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0377\n",
      "Epoch 6/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0364\n",
      "Epoch 7/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0357\n",
      "Epoch 8/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0354\n",
      "Epoch 9/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0352\n",
      "Epoch 10/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0351\n",
      "Epoch 11/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0351\n",
      "Epoch 12/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 13/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 14/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 15/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 16/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 17/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 18/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 19/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 20/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 21/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 22/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 23/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 24/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 25/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 26/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 27/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 28/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 29/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 30/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 31/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 32/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 33/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 34/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 35/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 36/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 37/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 38/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 39/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 40/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 41/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 42/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 43/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 44/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 45/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 46/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 47/100\n",
      "2553/2553 [==============================] - 0s 19us/step - loss: 0.0350\n",
      "Epoch 48/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 49/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 50/100\n",
      "2553/2553 [==============================] - 0s 19us/step - loss: 0.0350\n",
      "Epoch 51/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 52/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 53/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 54/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 55/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 56/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 57/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 58/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 59/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 60/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 61/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 62/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 63/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 64/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 65/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 66/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 67/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 68/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 69/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 70/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 71/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 72/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 73/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 74/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 75/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 76/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 77/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 78/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 79/100\n",
      "2553/2553 [==============================] - 0s 18us/step - loss: 0.0350\n",
      "Epoch 80/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 81/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 82/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 83/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 84/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 85/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 86/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 87/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 88/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 89/100\n",
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 90/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 91/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 92/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 93/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 94/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 95/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2553/2553 [==============================] - 0s 16us/step - loss: 0.0350\n",
      "Epoch 97/100\n",
      "2553/2553 [==============================] - 0s 17us/step - loss: 0.0350\n",
      "Epoch 98/100\n",
      "2553/2553 [==============================] - 0s 15us/step - loss: 0.0350\n",
      "Epoch 99/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n",
      "Epoch 100/100\n",
      "2553/2553 [==============================] - 0s 14us/step - loss: 0.0350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81fa512d90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression\n",
    "\n",
    "#structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(1, input_shape =[4]), #input\n",
    "])\n",
    "\n",
    "#how to train the model\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "              optimizer = kb.optimizers.SGD())\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(X,y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a linear regression model using LinearRegression and fitting on X and y (no need for model validation here)\n",
    "\n",
    "lr = LinearRegression().fit(X,y)\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.10874633],\n",
       "        [ 0.09833797],\n",
       "        [-0.03422379],\n",
       "        [ 0.03503103]], dtype=float32),\n",
       " array([0.47110033], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get weights from Neural Network\n",
    "model.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10888319,  0.09749521, -0.03496317,  0.03511044])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the coef_ and intercept_ from your linear regression model\n",
    "lr.coef_\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question*\n",
    "What happens to the weights from our neural net as you **increase** the number of epochs (compare to the coefs from the linear regression model)?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "they're very similar already, and get more and more similar as you increase the epochs (which makes sense as they both have the same structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter Bloat\n",
    "\n",
    "Remember that a densely connected layer (`Dense()` in keras) is connected to EVERY node in the layer before and after it. The parameters can add up QUICKLY. Looking at the image of a densely connected, deep neural network below, try to calculate how many parameters (weights + bias) need to be estimates for that neural network (it's okay if you're off by a litte).\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=19mh5qaqStcZzxir6fSWHkaiEtwMiVIVT\" width = 600px />\n",
    "\n",
    "### *Question*\n",
    "\n",
    "What do you think can happen when you have a ton of parameters and only a little data?\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "We can get overfitting. NN's are SUPER flexible, which is why you can do so many cool things with them. However, with increased flexibility/complexity comes a risk of overfitting. (however check out \"Double Descent\" if you're interested, it's the idea that with DEEP learning, that bias/variance trade off doesn't hold in the exact same way....)\n",
    "\n",
    "\n",
    "# 3. Building a Deep Neural Net\n",
    "\n",
    "Run the following model on the dataset `nn`. You can use `nn_test` as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10) (50, 10)\n"
     ]
    }
   ],
   "source": [
    "nn = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/NN.csv\")\n",
    "nn_test = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/NN_test.csv\")\n",
    "\n",
    "X = nn[[\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\"]]\n",
    "y = nn[[\"V10\"]]\n",
    "\n",
    "X_test = nn_test[[\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\"]]\n",
    "y_test = nn_test[[\"V10\"]]\n",
    "\n",
    "\n",
    "print(nn.shape, nn_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Model with layers with 9,7,5,3,2,1 nodes respectively. Fill in the appropriate numbers to relace the `???`s. I've done the 9 and 7 for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0519\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.9916\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.9589\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.9303\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.9091\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.8929\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.8794\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 0.8544\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.8363\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.8203\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.7991\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.7838\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.7556\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.7353\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.7154\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.6948\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.6716\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6515\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.6344\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.6241\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6015\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.5886\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.5770\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.5652\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.5549\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.5460\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.5269\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.5168\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.5096\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.5039\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.4988\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.4948\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.4904\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.4861\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.4771\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.4749\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.4692\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 43us/step - loss: 0.4660\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.4609\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.4567\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.4541\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.4514\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.4439\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.4409\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.4402\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.4354\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.4302\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.4205\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.4173\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.4133\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.4115\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.4102\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.4114\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.4015\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.4032\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.3971\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.3943\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.3919\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.3927\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.3839\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3843\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.3938\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.3859\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.3780\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.3714\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.3710\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.3659\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 46us/step - loss: 0.3671\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.3647\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.3598\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.3558\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.3543\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.3499\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 41us/step - loss: 0.3493\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.3433\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.3423\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.3397\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.3383\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.3368\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.3351\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.3383\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.3307\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.3276\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 46us/step - loss: 0.3286\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.3358\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.3272\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.3218\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.3214\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3156\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.3157\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 46us/step - loss: 0.3142\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 46us/step - loss: 0.3119\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.3161\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 42us/step - loss: 0.3177\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.3190\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.3201\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.3128\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 46us/step - loss: 0.3107\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.3086\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 45us/step - loss: 0.3148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81fb1779a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DEEP MODEL\n",
    "#structure of the model\n",
    "model2 = kb.Sequential([\n",
    "    kb.layers.Dense(7, input_shape =[9]), #input\n",
    "    kb.layers.Dense(5),\n",
    "    kb.layers.Dense(3),\n",
    "    kb.layers.Dense(2),\n",
    "    kb.layers.Dense(1) #output\n",
    "])\n",
    "#how to train the model\n",
    "model2.compile(loss = \"mean_squared_error\",\n",
    "              optimizer = kb.optimizers.SGD())\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model2.fit(X,y, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `nn` and `nn_test`, and calculate the train and test MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3217318957089084\n",
      "0.373792789025582\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "print(mean_squared_error(model2.predict(X),y))\n",
    "print(mean_squared_error(model2.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Random NN Topics That are Cool\n",
    "* Deep Neural Networks (NN's that have 2+ hidden layers)\n",
    "* Dropout (a way to regularize NNs)\n",
    "* Double Descent (You won't believe what this means for bias/variance tradedoff!!!)\n",
    "* Autoencoders (NN's that do non-linear PCA)\n",
    "* Generative Adversarial Networks (GANs; builds a model that can generate fake data, like faces, or paintings)\n",
    "* Recurrent Neural Networks (used for time series like sentences, music, stocks...even[harry potter](https://www.digitaltrends.com/cool-tech/harry-potter-ai-story/))\n",
    "* StyleGAN\n",
    "* Convolutional Neural Networks (often used for images, or other spatial data)\n",
    "* Shap values (a way to estimate the effect of different predictors in the NN)\n",
    "\n",
    "Check out [this video](https://www.youtube.com/watch?v=JBlm4wnjNMY) I wrote for Crash Course about Neural Nets."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
