{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.7g'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB # Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "from sklearn.model_selection import cross_val_score # cross validation metrics\n",
    "from sklearn.model_selection import cross_val_predict # cross validation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#set precision to get rid of some scientific notation\n",
    "%precision %.7g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## 0. Together\n",
    "\n",
    "\n",
    "### 0.0 Probability and Conditional Probability\n",
    "\n",
    "#### *Question*\n",
    "What is the difference between a conditional probability and a regular probability (for example $P(dog)$ vs $P(dog | kids)$)?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n",
    "Using the table below, how would we calculate $P(dog)$? $P(dog | kids)$?\n",
    "\n",
    "|           | dog | kid |\n",
    "|-----------|-----|-----|\n",
    "| Person 1  | 1   | 1   |\n",
    "| Person 2  | 1   | 1   |\n",
    "| Person 3  | 1   | 0   |\n",
    "| Person 4  | 1   | 1   |\n",
    "| Person 5  | 1   | 0   |\n",
    "| Person 6  | 1   | 1   |\n",
    "| Person 7  | 0   | 0   |\n",
    "| Person 8  | 0   | 1   |\n",
    "| Person 9  | 0   | 1   |\n",
    "| Person 10 | 0   | 1   |\n",
    "\n",
    "Using the table below, how would we calculate $P(dog | kids, over20)$?\n",
    "\n",
    "|           | dog | kid | over20 |\n",
    "|-----------|-----|-----|--------|\n",
    "| Person 1  | 1   | 1   | 1      |\n",
    "| Person 2  | 1   | 1   | 0      |\n",
    "| Person 3  | 1   | 0   | 0      |\n",
    "| Person 4  | 1   | 1   | 1      |\n",
    "| Person 5  | 1   | 0   | 1      |\n",
    "| Person 6  | 1   | 1   | 1      |\n",
    "| Person 7  | 0   | 0   | 1      |\n",
    "| Person 8  | 0   | 1   | 0      |\n",
    "| Person 9  | 0   | 1   | 0      |\n",
    "| Person 10 | 0   | 1   | 1      |\n",
    "\n",
    "### 0.1 Naive\n",
    "Naive Bayes is a classification algorithm which assumes (incorrectly) that within a group/class, the probability of a combination of predictor values (like $P(diabetic, obese, smoker)$) is equal to the product of the individual predictor probabilities. In other words, it assumes that they are *independent* and that knowing someone is a smoker does *not* affect the probability of being diabetic. In mathematical terms, for example:\n",
    "\n",
    "$$P(D,O,S) = P(D) * P(O) * P(S)$$\n",
    "\n",
    "In real life we know that this independence is very unlikely (hence: *naive*). But it turns out that this inapproporiate assumption doesn't usually have a huge effect on the accuracy of the model, and it saves a LOT of computational time because we can simply calculate independent probabilities and multiply them, rather than calculating complex conditional probabilities.\n",
    "\n",
    "### 0.2 Bayes\n",
    "The Bayes part of the Naive Bayes algorithm refers to the fact that we calculate \"scores\" that measure how likely a data point is to belong to some class, $C$. These \"scores\" are proportional to the probability of a data point belonging to class $C$. Once we have a \"score\" for each possible category, we choose whichever category has the highest score. \n",
    "\n",
    "The \"score\" is based on Bayes' Theory which says:\n",
    "\n",
    "$$P(category | data) \\propto \\underbrace{P(Data | Category)}_{\\text{How common this combination of predictors is for that Category}^1} * \\underbrace{P(Category)}_\\text{How common that category is in the dataset}$$\n",
    "\n",
    "\n",
    "$^1$\n",
    "For example, what is the probability that someone is diabetic, obsese, and a smoker given that they have heart disease.\n",
    "\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "$$P(A | B) = \\frac{P(B| A) * P(A)}{P(B)}$$\n",
    "\n",
    "We don't care about the denominator...\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1qUXP_sITPnv4JNIjYCOkwUNvKZRuYn_6\" width = 200px />\n",
    "\n",
    "so it becomes...\n",
    "\n",
    "$$P(A | B) \\propto P(B| A) * P(A)$$\n",
    "\n",
    "or in other words...\n",
    "\n",
    "$$P(category | \\text{X variables}) \\underbrace{\\propto P(\\text{X variables}| category)}_\\textbf{how likely is it to see this pattern in this category?} * \\underbrace{P(category)}_\\textbf{how likely is this category?}$$\n",
    "\n",
    "\n",
    "### 0.3 Different NB functions in `sklearn`\n",
    "In sklearn there are 3 main functions you can use to perform Naive Bayes:\n",
    "\n",
    "* `GaussianNB()`: Assumes that features follow a Normal/Gaussian Distribution.\n",
    "* `BernoulliNB()`: Assumes features are binary (0/1)\n",
    "* `CategoricalNB()`: Assumes features are discrete categories (can have more than 2 categories)\n",
    "\n",
    "This means that if your features are continuous you'd use `GaussianNB()`, if they are only binary, use `BernoulliNB()` and if they are only Categorical, use `CategoricalNB()`. In practice, we'll often use either `GaussianNB()` or `CategoricalNB()` (since `CategoricalNB()` can also handle it when we have binary + categorical).\n",
    "\n",
    "This means that computationally, we cannot have both continuous + categorical predictors in one sklearn NB model. (There are workarounds for this: see [here](https://stackoverflow.com/questions/14254203/mixing-categorial-and-continuous-data-in-naive-bayes-classifier-using-scikit-lea), but for now, we'll be using only one or the other).\n",
    "\n",
    "\n",
    "### Categorical NB example\n",
    "This dataset looks at customer behavior (whether they clicked on 2 emails, and whether they've made a purchase in the past) to predict whether the customer will make a purchase on Black Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicked_email1</th>\n",
       "      <th>clicked_email2</th>\n",
       "      <th>made_past_purchase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>made_bf_purchase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.654545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  clicked_email1  clicked_email2  made_past_purchase\n",
       "made_bf_purchase                                                    \n",
       "0                       0.170000        0.260000            0.310000\n",
       "1                       0.272727        0.218182            0.654545"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/made_purchase.csv\")\n",
    "\n",
    "df.groupby(\"made_bf_purchase\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "made_bf_purchase\n",
       "0    0.645161\n",
       "1    0.354839\n",
       "Name: made_bf_purchase, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"made_bf_purchase\")[\"made_bf_purchase\"].count()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made A Purchase 0.043159981\n",
      "No Purchase     0.036854221\n"
     ]
    }
   ],
   "source": [
    "# probabilities by hand \n",
    "\n",
    "# someone didn't click email 1, clicked email 2, and has made a past purchase. \n",
    "# should we classify them as making or not making a black friday purchase?\n",
    "# [0, 1, 1]\n",
    "\n",
    "made_score = (1-0.17)*(0.26)*(0.31)* 0.645161\n",
    "didnt_score = (1-0.272727)*(0.218182)*(0.654545)* 0.354839\n",
    "\n",
    "print(\"Made A Purchase\", f\"{made_score:.9f}\")\n",
    "print(\"No Purchase    \", f\"{didnt_score:.9f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Naive Bayes By Hand\n",
    "\n",
    "### 1.1 Calculating Probabilities for Each Category\n",
    "\n",
    "The dataframe `d` below, is a (fake) dataset that we'll use to predict whether someone owns a home or not (the `own` column). For each outcome category (own-`1`, not own-`0`) calculate the probability of having a `1` in each of the predictor categories (having an income > 100k, being over 40, having kids, and having more than one income).\n",
    "\n",
    "\n",
    "Store these probabilities in a dataframe. The dataframe should look like the table below, but with the actual probabilities instead of 1's.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1imX0dbPjiEy56kruM8c86A3wA1EqpA8Q\" width = 250px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/HomeOwnership.csv\")\n",
    "d.head()\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Predicting Category\n",
    "Using the formula we learned in the Naive Bayes lecture, choose which category (own-`1` or not own-`0`) the following two people should be classified as:\n",
    "\n",
    "| incomeOver100k | ageOver40 | kids | morethan1Income |\n",
    "|----------------|-----------|------|-----------------|\n",
    "| 0              | 1         | 1    | 0               |\n",
    "| 1              | 1         | 0    | 1               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build a NB in sklearn\n",
    "\n",
    "#### *Question*\n",
    "Now, using d, build a naive bayes model using `d` (no need for model validation here). Then use the `.predict()` function to predict the category for the two people from 1.2. Does the models predicted category match the one you did by hand?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Use BernoulliNB or CategoricalNB since we have categorical variables\n",
    "# hint: to predict a single data point, use: data_point = np.array(dp).reshape(1,-1), where dp is a list with\n",
    "# the predictor values, and then call .predict(data_point) on your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build a CONTINUOUS NB in sklearn\n",
    "\n",
    "While we won't do the math by hand for the continous (Gaussian) version of Naive Bayes, let's practice running it in sklearn.\n",
    "\n",
    "Using the `diabetes` dataset, create and fit a `GaussianNB()` model to predict whether or not someone has diabetes (`1`-diabetes, `0`-no diabetes). Use Train Test Split an evaluate how well your model does on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/diabetes2.csv\")\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Use GaussianNB because we have all continuous predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Being Naive is...good!\n",
    "\n",
    "We mentioned in lecture why the naive assumption in NB is useful, computationally. But now, it's your turn to experience it first hand! Using the LARGER home ownership dataset `d2`, first calculate the probability $P(1,0,1,1)$ (where `[1,0,1,1]` represents a person's values for the 4 predictors, `incomeOver100k`, `ageOVer40`, `kids`, and `morethan1Income`) the **naive** way for *both* home owners and non-owners. \n",
    "\n",
    "$$ P(A,B,C,D) = P(A)*P(B)*P(C)*P(D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/HomeOwnership2.csv\")\n",
    "d2.head()\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "#p(1,0,1,1) for homeowners\n",
    "\n",
    "#p(1,0,1,1) for non-homeowners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate $P(1,0,1,1 | \\text{own})$ (where `[1,0,1,1]` represents a person's values for the 4 predictors, `incomeOver100k`, `ageOVer40`, `kids`, and `morethan1Income`) the **regular** way for *both* home owners and non-owners. \n",
    "\n",
    "Using the *chain rule* of probabilities, the probability of multiple events, $P(A,B,C,D)$ is equal to:\n",
    "\n",
    "$$P(A,B,C,D)= P(A|B,C,D)*P(B|C,D)*P(C|D)*P(D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "#p(1,0,1,1) for homeowners\n",
    "\n",
    "#p(1,0,1,1) for non-homeowners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much simpler the naive way is?? and this is a TINY dataset with very few features."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
