{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1qEgwsGeMTB7JwqGjMxeBeFKrlHch6Hqx\" width=500px />\n",
    "\n",
    "\n",
    "## Activity\n",
    "In our lecture, we talked about *cohesion* as it relates to clusters. Cohesion refers to how similar members of a cluster are to each other. High cohesion means very similar, low cohesion means very dissimilar. In your groups, find a topic (music, classes, food, hobbies, sports team alligences, books, tiktok, youtube, video games) that your group is *cohesive* in. Find another topic that your group is NOT cohesive in.\n",
    "\n",
    "## K-Means\n",
    "\n",
    "K-Means is an unsupervised ML algorithm. **Unsupervised ML** does not have \"truth labels\" for a model to learn. Rather, we use these algorithms to find what we call \"latent structure\" in our data. The most common form of \"structure\" we're looking for is groups (aka, clusters). \n",
    "\n",
    "K-means is one of the most simple clustering algorithms out there. It has 4 steps:\n",
    "\n",
    "1. Randomly (or, in order to make convergence quicker, cleverly) choose K centroids in the feature space\n",
    "2. Assign each data point to the centroid/cluster closest to it\n",
    "3. Recalculate each centroid by taking the mean (for each predictor) of all the data points in each cluster.\n",
    "4. Repeat Steps 2 and 3 until convergence\n",
    "    - either cluster assignments don't change from step to step OR\n",
    "    - the centroid doesn't change much from step to step\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1RHRfcPIjIZ_-IMOE00gyzVadlaGxXPh8\" width=350px />\n",
    "\n",
    "One thing to keep in mind with K-Means is that is assumes *spherical* variance within each cluster. That means that K-means behaves as if--within each cluster--all predictors have the same variance. Roughly, this means that we could easily draw a sphere (or circle) around each of our clusters, even if it's not perfectly spherical.\n",
    "\n",
    "## K-Means++\n",
    "The inital cluster centers chosen in step 1 above can have a big impact on the final clusters that K-Means returns. To overcome this, one proposed solution is **K-means++** which initializes cluster centers in a smart way:\n",
    "\n",
    "- randomly select the first cluster center by randomly selecting a single data point\n",
    "- for each data point, calculate the distance between it and the closest centroid\n",
    "- randomly select another cluster center so that points *furthest away* from the current centroids have the highest probability of being selected\n",
    "- repeat this process until $k$ cluster centers have been chosen\n",
    "\n",
    "\n",
    "By choosing cluster centers (centroids) far away from each other, we're most likely to get points that are a part of different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super vs. Unsuper\n",
    "K-Means is our first clustering algorithm. Clustering is *unsupervised machine learning*. Based on the definition of *supervised machine learning* we talked about before, what makes k-means unsupervised?\n",
    "\n",
    "### Supervised ML\n",
    "Notice in the above code, our workflow is broadly:\n",
    "\n",
    "1. load in data, create X's and y's (including model validation)\n",
    "2. Create Empty Model + Pipeline\n",
    "3. Fit your model\n",
    "4. Assess your model (using `.predict()` + some performance metric)\n",
    "\n",
    "\n",
    "### Unsupervised ML\n",
    "In Unsupervised ML, we don't HAVE the \"correct answers\" for our model. Unsupervised ML focuses on finding underlying *structure* in the data, e.g. clusters. There's no \"right\" answer for these models. \n",
    "\n",
    "Because there isn't a \"right\" answer for us to compare our results to, we usually don't worry about Model Validation, nor do we care about data leakage. So our workflow is a little simpler.\n",
    "\n",
    "1. load in our data\n",
    "2. Create Empty Model + Pipeline\n",
    "3. Fit your Model\n",
    "4. Assess your model (using `.predict()`/`labels_` and by looking at plots, or something like a silhouette score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STEP 1. ####\n",
    "bey = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/Beyonce_data.csv\")\n",
    "bey.head()\n",
    "\n",
    "predictors = [\"energy\", \"danceability\", \"valence\"]\n",
    "\n",
    "\n",
    "X = bey[predictors]\n",
    "\n",
    "#### STEP 2. ####\n",
    "z = make_column_transformer((StandardScaler(), predictors),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "#### STEP 3. ####\n",
    "km = KMeans(n_clusters = 4)\n",
    "pipe = Pipeline([(\"z\", z), (\"clust\", km)])\n",
    "\n",
    "#### STEP 4. ####\n",
    "labels = pipe.fit_predict(X)\n",
    "\n",
    "#### STEP 5. ####\n",
    "print(silhouette_score(X, pipe.predict(X)))\n",
    "X[\"clusters\"] = labels\n",
    "\n",
    "print(ggplot(X, aes(x = \"energy\", y = \"danceability\", color = \"factor(clusters)\" )) +\n",
    "      geom_point() + theme_minimal() +\n",
    "     labs(x = \"Energy\", y = \"Danceability\", title = \"KMeans Clustering Results for K = 4\",\n",
    "         color = \"Clusters\"))\n",
    "print(ggplot(X, aes(x = \"energy\", y = \"valence\", color = \"factor(clusters)\" )) +\n",
    "      geom_point() + theme_minimal() +\n",
    "     labs(x = \"Energy\", y = \"Valence\", title = \"KMeans Clustering Results for K = 4\",\n",
    "         color = \"Clusters\"))\n",
    "print(ggplot(X, aes(x = \"valence\", y = \"danceability\", color = \"factor(clusters)\" )) +\n",
    "      geom_point() + theme_minimal() +\n",
    "     labs(x = \"Valence\", y = \"Danceability\", title = \"KMeans Clustering Results for K = 4\",\n",
    "         color = \"Clusters\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Model Validation isn't a part of our workflow anymore. Also notice that there are no `y` variables. When we `.fit()`, we only give it an `X`! This is all because KMeans is an *unsupervised* ML Algorithm.\n",
    "\n",
    "### Another Example (choosing `k` using the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/wineLARGE.csv\")\n",
    "\n",
    "# drop and reset rows\n",
    "wine.dropna(inplace = True)\n",
    "wine.reset_index(inplace = True)\n",
    "\n",
    "# grab data we want to cluster\n",
    "feats = [\"citric.acid\", \"residual.sugar\"]\n",
    "\n",
    "X = wine[feats]\n",
    "\n",
    "# create empty model\n",
    "z = make_column_transformer((StandardScaler(), feats),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "metrics = {\"SSE\": [], \"sil\": [], \"k\": []}\n",
    "\n",
    "for i in range(2,20):\n",
    "    km = KMeans(i)\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", z),\n",
    "        (\"km\", km)\n",
    "    ])\n",
    "    \n",
    "    labels = pipe.fit_predict(X[feats])\n",
    "    sil = silhouette_score(X[feats], labels)\n",
    "    sse = pipe.named_steps[\"km\"].inertia_\n",
    "\n",
    "    metrics[\"SSE\"].append(sse)\n",
    "    metrics[\"sil\"].append(sil)\n",
    "    metrics[\"k\"].append(i)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "\n",
    "print(ggplot(df, aes(x = \"k\", y = \"SSE\")) +\n",
    "  geom_line() + theme_minimal() +\n",
    "    labs(x = \"K\", y = \"Within Cluster SSE\",\n",
    "         title = \"Within Cluster SSE for Different Ks\"))\n",
    "\n",
    "print(ggplot(df, aes(x = \"k\", y = \"sil\")) +\n",
    "  geom_line() + theme_minimal() +\n",
    "    labs(x = \"K\", y = \"Mean Silhouette Score\",\n",
    "         title = \"Silhouette Scores for Different Ks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(5)\n",
    "pipe = Pipeline([\n",
    "      (\"pre\", z),\n",
    "      (\"km\", km)\n",
    "])\n",
    "labels = pipe.fit_predict(X)\n",
    "\n",
    "X[\"cluster\"] = labels\n",
    "print(ggplot(X, aes(x = \"citric.acid\", y = \"residual.sugar\", color = \"factor(cluster)\")) +\n",
    "      geom_point() +\n",
    "      theme_minimal() + \n",
    "      scale_color_discrete(name = \"Cluster\") + \n",
    "      labs(x = \"Citric Acid\", \n",
    "           y = \"Residual Sugar\",\n",
    "           title = \"5 Cluster Solution\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork\n",
    "\n",
    "## K-Means from Scratch\n",
    "\n",
    "Let's write our own simplified K-means function. Your function, `KM()` should take in two arguments:\n",
    "\n",
    "- `df` a dataframe with all of your data.\n",
    "- `k` the number of clusters to fit.\n",
    "\n",
    "and apply K-Means to it. Remember that the steps of K means are:\n",
    "\n",
    "**1**. DONE FOR YOU. Randomly select k centroids.\n",
    "- I recommend choosing `k` random data points from `df`. You can do this by using `np.random.choice(range(0,df.shape[0]), k)` to select the indices for `k` randomly selected rows. THEN use those indices to grab the chosen rows from df and store them.\n",
    "    \n",
    "**2**. Assign each data point from `df` to the closest centroid.\n",
    "- You'll need to calculate the distance between each data point and each centroid. Perhaps look at the KNN classwork to see how to do that using `np.linalg.norm()` (see Hint 3).\n",
    "    - I recommend storing cluster/centroid membership by having a dictionary with one key for each cluster/centroid, and the value is a list of row indices pertaining to the data points in each cluster (see HINT 1 for an example of this)\n",
    "    \n",
    "**3**. Re-calculate the cluster mean/centroid\n",
    "- For each centroid/cluster, find the mean value for each predictor/feature by taking the mean for that feature from all the data points assigned to the centroid/cluster.(see Hint 2)\n",
    "    \n",
    "**4**. DONE FOR YOU. Repeat Steps 2-3 until the change in centroid positions are all less than 0.0001\n",
    "- in other words, calculate how far each centroid moved. If all of them moved less than 0.0001 units, then stop.\n",
    "    \n",
    "**5**. DONE FOR YOU. Return the cluster assignments by returning the dictonary of the clusters and their memberships that you create in #2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT 1:\n",
    "\n",
    "You can store your cluster memberships like this (in this case k = 3, and there are only 20 datapoints, but your function should take any k, and any number of data points):\n",
    "\n",
    "```\n",
    "clust = {0: [0,7,4,5,12,18,20],\n",
    "         1: [10,8,3,2,14,17,19],\n",
    "         2: [1,6,7,9,11,13,15,16]}\n",
    "```\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT 2:\n",
    "\n",
    "If a cluster contained the following data points:\n",
    "\n",
    "|           | X1 | X2 | X3 |\n",
    "|-----------|----|----|----|\n",
    "| Person 1  | 5  | 2  | 9  |\n",
    "| Person 2  | 2  | 3  | 2  |\n",
    "| Person 3  | 1  | 6  | 1  |\n",
    "| Person 4  | 7  | 1  | 4  |\n",
    "| Person 5  | 3  | 2  | 5  |\n",
    "| Person 6  | 1  | 1  | 8  |\n",
    "| Person 7  | 7  | 0  | 6  |\n",
    "| Person 8  | 0  | 7  | 2  |\n",
    "| Person 9  | 2  | 3  | 7  |\n",
    "| Person 10 | 4  | 6  | 1  |\n",
    "\n",
    "\n",
    "Then the centroid for that cluster would be [a,b,c] where a, b, and c are the means of each column X1, X2, and X3:\n",
    "\n",
    "a = (5 + 2 + 1 + 7 + 3 + 1 + 7 + 0 + 2 + 4)/10 \n",
    "\n",
    "b = (2 + 3 + 6 + 1 + 2 + 1 + 0 + 7 + 3 + 6)/10\n",
    "\n",
    "c = (9 + 2 + 1 + 4 + 5 + 8 + 6 + 2 + 7 + 1)/10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT 3:\n",
    "\n",
    "To calculate the distance between two vectors, you can use:\n",
    "\n",
    "```\n",
    "distance_ab = np.linalg.norm(a-b)\n",
    "\n",
    "```\n",
    "\n",
    "where `a` and `b` are the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT 4:\n",
    "\n",
    "The `np.argmin()` function takes in a list (or array) of values, and returns the *index* of the smallest one.\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "my_list = [1,6,2,5,0]\n",
    "\n",
    "np.argmin(my_list)\n",
    "```\n",
    "\n",
    "this code would return 4, because the smallest value (0) in `my_list` is at index 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_centroids(df,k):\n",
    "    # DONE FOR YOU, DON'T CHANGE ANYTHING\n",
    "\n",
    "    '''\n",
    "    use the row nums in c to grab the k data points and store them in centroids.\n",
    "    centroids should be a list of rows (each row contains the data point you chose)\n",
    "    to be a cluster center\n",
    "    '''\n",
    "    \n",
    "    c = np.random.choice(range(0,df.shape[0]), k)\n",
    "    centroids = [df.iloc[l] for l in c]\n",
    "    return(centroids)\n",
    "\n",
    "def choose_closest_cluster(centroids, df, k):\n",
    "    '''\n",
    "    Create a dictionary, clust, that stores the row numbers of all the data points \n",
    "    in each cluster. \n",
    "\n",
    "    e.g. {0: [0,7], 1: [1,6,12,13], 2: [4,5], 3: [3,8,9], 4: [2,10,11]}\n",
    "\n",
    "    Loop through each data point, and calculate the distance between that data point\n",
    "    and all the cluster centers (centroids), and store them in a list. Remember\n",
    "    np.linalg.norm()!\n",
    "     \n",
    "    Then, use np.argmin() to figure out which centroid is the closest, and assign\n",
    "    the data point to that cluster by adding it's row number (stored in dataPoint)\n",
    "    to the clust dictionary.\n",
    "    '''\n",
    "\n",
    "    # creating empty clust dictionary\n",
    "    clust = {}\n",
    "    for c in range(0,k):\n",
    "        clust[c] = []\n",
    "    \n",
    "    # loop through each row in df\n",
    "    for dataPoint in range(0,df.shape[0]):\n",
    "        pass\n",
    "        # calculate the distance between current data point and each centroid\n",
    "\n",
    "\n",
    "        # find the centroid that's closest\n",
    "\n",
    "\n",
    "        # add dataPoint to that cluster\n",
    "\n",
    "\n",
    "    return(clust)\n",
    "\n",
    "def recalculate_cluster_mean(clust, df, k):\n",
    "    '''\n",
    "    This function takes in a dictionary of cluster memberships and the data\n",
    "    and returns the NEW cluster centers, stored in new_centroids. Cluster centers\n",
    "    are calculated by taking the mean of each feature for all the data points in\n",
    "    each cluster.\n",
    "    \n",
    "    new_centroids should be a list of arrays that represent the new cluster centers.\n",
    "    \n",
    "    Hint: what happens when you call .mean() on a dataframe?\n",
    "    '''\n",
    "\n",
    "    new_centroids = [[] for c in range(0,k)] #create an empty list of k 0's\n",
    "\n",
    "    for c in range(0,k):\n",
    "        # calculate the center/mean of cluster c\n",
    "        pass\n",
    "\n",
    "    # turn our list into an array\n",
    "    new_centroids = np.array(new_centroids)\n",
    "\n",
    "    return(new_centroids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KM(df,k):\n",
    "    \n",
    "    # 1. randomly select k centroids\n",
    "    centroids = choose_centroids(df,k)\n",
    "\n",
    "    converged = False # has the algorithm converged yet?\n",
    "    while not converged: # until the centroids stop moving\n",
    "\n",
    "        # 2. assign data points to a cluster\n",
    "        clust = choose_closest_cluster(centroids,df, k)\n",
    "\n",
    "        # 3. re-calculate the center/centroid of each cluster\n",
    "        new_centroids = recalculate_cluster_mean(clust,df, k)\n",
    "        \n",
    "        # 4. check whether you can stop iterating by checking whether the\n",
    "        # distance between the previous position and current position is\n",
    "        # less than 0.0001 for all k centroids.\n",
    "\n",
    "        # calculate the distance between the old centroid values, and new_centroids values\n",
    "        change = np.array([np.linalg.norm(centroids[i]-new_centroids[i]) for i in range(0,k)])\n",
    "        \n",
    "        # check whether all of them moved less than 0.0001 units.\n",
    "        converged = np.all(change < 0.0001)\n",
    "\n",
    "        # set new_centroids to be established centroids\n",
    "        centroids = new_centroids\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # 5. Return cluster memberships dictionary, the structure\n",
    "    # should look like this (but can have different k, and different\n",
    "    # assignments depending on data/starting centroids/chosen )\n",
    "    # {0: [55, 72, 76, 85, 89, 93, 100, 104, 105, 107, 110, 119,\n",
    "#          123, 132, 144, 201, 202, 203, 204, 205, 206, 207, 209,\n",
    "#          210, 212, 213, 214, 215, 217, 218, 220, 221, 222, 223,\n",
    "#          225, 226, 227, 228, 229, 231, 232, 233, 234, 237, 238,\n",
    "#          241, 243, 245, 246, 247, 248, 249],\n",
    "#     1: [8, 47, 102, 103, 111, 114, 117, 120, 126, 129, 131, 136,\n",
    "#          141, 142, 143, 145, 146, 148],\n",
    "#     2: [51, 101, 106, 108, 109, 112, 113, 115, 116, 118, 121,\n",
    "#           122, 124, 125, 127, 128, 130, 133, 134, 135, 137, 138,\n",
    "#           139, 140, 147, 149, 200, 219, 224, 230, 235, 239, 240,\n",
    "#           242, 244],\n",
    "#     3: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160,\n",
    "#           161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
    "#           172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
    "#           183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193,\n",
    "#           194, 195, 196, 197, 198, 199, 216, 236],\n",
    "#     4: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16,\n",
    "#           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
    "#           31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n",
    "#           45, 46, 48, 49, 50, 52, 53, 54, 56, 57, 58, 59, 60, 61,\n",
    "#           62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77,\n",
    "#           78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 94,\n",
    "#           95, 96, 97, 98, 99, 208, 211]}\n",
    "    return(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/programmers3.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your K-Means Function\n",
    "\n",
    "Now that you have done the incredibly impressive work of writing (with some help!) your own K-means function. Let's use it and compare the results to what we'd get from `sklearn`!\n",
    "\n",
    "First, use your OWN function `KM()` to do K-means on `data` with k = 5. Then generate the cluster assingments using the code provided. Then make a ggplot scatterplot of your clusters.\n",
    "\n",
    "Second, use sklearn's `KMeans()` function to do K-means on `data` with k = 5. Then generate the cluster assignments using `.predict()`. Then make a ggplot scatterplot of your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means\n",
    "\n",
    "# create features list\n",
    "feats = [\"py\", \"r\"]\n",
    "\n",
    "# z score object\n",
    "z = make_column_transformer((StandardScaler(), feats),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "# store z scored data in another data frame\n",
    "data_z = data.copy()\n",
    "data_z[feats] = pd.DataFrame(z.fit_transform(data[feats]))\n",
    "\n",
    "# use your function\n",
    "clusters = KM(data_z[feats], 5)\n",
    "\n",
    "# generate assignments\n",
    "assignments = np.array([999 for row in range(0, data_z.shape[0])])\n",
    "\n",
    "for cluster in clusters:\n",
    "    assignments[clusters[cluster]] = cluster\n",
    "    \n",
    "data[\"assignments_ME\"] = assignments\n",
    "\n",
    "\n",
    "# create ggplot scatter plot of data, using x, y and color = \"assignments_ME\"\n",
    "(ggplot(data, aes(\"py\", \"r\", color = \"factor(assignments_ME)\")) +\n",
    "  geom_point() + theme_minimal() + \n",
    "  scale_color_discrete(name = \"Clusers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING SKLEARN\n",
    "\n",
    "# create kmeans model\n",
    "\n",
    "\n",
    "# fit kmeans model\n",
    "# get assignments\n",
    "\n",
    "assignments= ###\n",
    "\n",
    "\n",
    "\n",
    "# add assignments to data\n",
    "data[\"assignments_SK\"] = assignments\n",
    "\n",
    "# create another ggplot scatter plot of data, using x, y and color = \"assignments_SK\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question*\n",
    "\n",
    "How do your results compare?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
